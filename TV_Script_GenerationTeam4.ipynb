{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled4.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "7L9Ykxg0wacQ"
      },
      "source": [
        "import helper\n",
        "data_dir = './Seinfeld_Scripts.txt'\n",
        "text = helper.load_data(data_dir)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BaewNFm4wp5Q",
        "outputId": "b6526887-f6eb-4830-9b6e-5f7942e0fb47"
      },
      "source": [
        "view_line_range = (0, 10)\n",
        "import numpy as np\n",
        "\n",
        "print('Dataset Stats')\n",
        "print('Roughly the number of unique words: {}'.format(len({word: None for word in text.split()})))\n",
        "\n",
        "lines = text.split('\\n')\n",
        "print('Number of lines: {}'.format(len(lines)))\n",
        "word_count_line = [len(line.split()) for line in lines]\n",
        "print('Average number of words in each line: {}'.format(np.average(word_count_line)))\n",
        "\n",
        "print()\n",
        "print('The lines {} to {}:'.format(*view_line_range))\n",
        "print('\\n'.join(text.split('\\n')[view_line_range[0]:view_line_range[1]]))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset Stats\n",
            "Roughly the number of unique words: 46367\n",
            "Number of lines: 109233\n",
            "Average number of words in each line: 5.544240293684143\n",
            "\n",
            "The lines 0 to 10:\n",
            "jerry: do you know what this is all about? do you know, why were here? to be out, this is out...and out is one of the single most enjoyable experiences of life. people...did you ever hear people talking about we should go out? this is what theyre talking about...this whole thing, were all out now, no one is home. not one person here is home, were all out! there are people trying to find us, they dont know where we are. (on an imaginary phone) did you ring?, i cant find him. where did he go? he didnt tell me where he was going. he must have gone out. you wanna go out you get ready, you pick out the clothes, right? you take the shower, you get all ready, get the cash, get your friends, the car, the spot, the reservation...then youre standing around, what do you do? you go we gotta be getting back. once youre out, you wanna get back! you wanna go to sleep, you wanna get up, you wanna go out again tomorrow, right? where ever you are in life, its my feeling, youve gotta go. \n",
            "\n",
            "jerry: (pointing at georges shirt) see, to me, that button is in the worst possible spot. the second button literally makes or breaks the shirt, look at it. its too high! its in no-mans-land. you look like you live with your mother. \n",
            "\n",
            "george: are you through? \n",
            "\n",
            "jerry: you do of course try on, when you buy? \n",
            "\n",
            "george: yes, it was purple, i liked it, i dont actually recall considering the buttons. \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IgcK2NWdw2qq",
        "outputId": "46e3473d-cb33-44cd-aa48-a733ea562068"
      },
      "source": [
        "\n",
        "import TestCase as tests\n",
        "from collections import Counter\n",
        "\n",
        "def create_lookup_tables(text):\n",
        "    word_count = Counter(text)\n",
        "    \n",
        "    sorted_vocab = sorted(word_count, key=word_count.get, reverse=True)\n",
        "    \n",
        "    int_to_vocab = {ii: word for ii, word in enumerate(sorted_vocab)}\n",
        "    vocab_to_int = {word: ii for ii, word in int_to_vocab.items()}\n",
        "\n",
        "    return (vocab_to_int, int_to_vocab)\n",
        "\n",
        "\"\"\"\n",
        "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
        "\"\"\"\n",
        "tests.test_create_lookup_tables(create_lookup_tables)\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tests Passed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C1Vd0NSxxcPd",
        "outputId": "9c340368-7684-4e8c-e8e5-07d6505101c7"
      },
      "source": [
        "def token_lookup():\n",
        "    tokens = dict()\n",
        "    tokens['.'] = '||period||'\n",
        "    tokens[','] = '||comma||'\n",
        "    tokens['\"'] = '||quotation_mark||'\n",
        "    tokens[';'] = '||semicolon||'\n",
        "    tokens['!'] = '||exclam_mark||'\n",
        "    tokens['?'] = '||question_mark||'\n",
        "    tokens['('] = '||left_par||'\n",
        "    tokens[')'] = '||right_par||'\n",
        "    tokens['-'] = '||dash||'\n",
        "    tokens['\\n'] = '||return||'\n",
        "    \n",
        "    return tokens\n",
        "tests.test_tokenize(token_lookup)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tests Passed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5mjDDoHyFx1"
      },
      "source": [
        "helper.preprocess_and_save_data(data_dir, token_lookup, create_lookup_tables)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hknTJS_HyMPC"
      },
      "source": [
        "import helper\n",
        "import TestCase as tests\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "int_text, vocab_to_int, int_to_vocab, token_dict = helper.load_preprocess()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULa5eBoeyRxV"
      },
      "source": [
        "import torch\n",
        "train_on_gpu = torch.cuda.is_available()\n",
        "if not train_on_gpu:\n",
        "    print('No GPU found. Please use a GPU to train your neural network.')"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CPHMxZrQyn_G",
        "outputId": "5e559cc5-66fd-4b51-e5c4-b72c3c965b15"
      },
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "\n",
        "def batch_data(words, sequence_length, batch_size):\n",
        "    X = []\n",
        "    y = []\n",
        "    \n",
        "    for i in range(0,len(words)-sequence_length):\n",
        "        X.append(words[i:i+sequence_length])\n",
        "        y.append(words[i+sequence_length])\n",
        "    \n",
        "    data = TensorDataset(torch.from_numpy(np.array(X, dtype=np.uint8)),\n",
        "                         torch.from_numpy(np.array(y, dtype=np.uint8)))\n",
        "    \n",
        "    loader = DataLoader(data, shuffle=True, batch_size=batch_size)\n",
        "    \n",
        "    return loader\n",
        "\n",
        "test_words =range(11)\n",
        "loader = batch_data(test_words,3,4)\n",
        "\n",
        "i = 0\n",
        "for x_data, y_data in loader:\n",
        "    i += 1\n",
        "    print('##Batch {}\\n'.format(i))\n",
        "    print(x_data.size())\n",
        "    print(x_data.data)\n",
        "    print('\\n')\n",
        "    print(y_data.size())\n",
        "    print(y_data.data)\n",
        "    print('\\n\\n\\n')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "##Batch 1\n",
            "\n",
            "torch.Size([4, 3])\n",
            "tensor([[7, 8, 9],\n",
            "        [6, 7, 8],\n",
            "        [2, 3, 4],\n",
            "        [1, 2, 3]], dtype=torch.uint8)\n",
            "\n",
            "\n",
            "torch.Size([4])\n",
            "tensor([10,  9,  5,  4], dtype=torch.uint8)\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "##Batch 2\n",
            "\n",
            "torch.Size([4, 3])\n",
            "tensor([[3, 4, 5],\n",
            "        [0, 1, 2],\n",
            "        [4, 5, 6],\n",
            "        [5, 6, 7]], dtype=torch.uint8)\n",
            "\n",
            "\n",
            "torch.Size([4])\n",
            "tensor([6, 3, 7, 8], dtype=torch.uint8)\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RpkkayHaytUH",
        "outputId": "01f9932f-6bf2-4365-8350-5b6f14086ab2"
      },
      "source": [
        "test_text = range(50)\n",
        "t_loader = batch_data(test_text, sequence_length=5, batch_size=10)\n",
        "\n",
        "data_iter = iter(t_loader)\n",
        "sample_x, sample_y = data_iter.next()\n",
        "\n",
        "print(sample_x.shape)\n",
        "print(sample_x)\n",
        "print()\n",
        "print(sample_y.shape)\n",
        "print(sample_y)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([10, 5])\n",
            "tensor([[19, 20, 21, 22, 23],\n",
            "        [23, 24, 25, 26, 27],\n",
            "        [44, 45, 46, 47, 48],\n",
            "        [40, 41, 42, 43, 44],\n",
            "        [28, 29, 30, 31, 32],\n",
            "        [21, 22, 23, 24, 25],\n",
            "        [30, 31, 32, 33, 34],\n",
            "        [35, 36, 37, 38, 39],\n",
            "        [ 6,  7,  8,  9, 10],\n",
            "        [33, 34, 35, 36, 37]], dtype=torch.uint8)\n",
            "\n",
            "torch.Size([10])\n",
            "tensor([24, 28, 49, 45, 33, 26, 35, 40, 11, 38], dtype=torch.uint8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R2YMueiYyxjy",
        "outputId": "697fb99e-8d01-49ed-9555-d6c5c6b38ffb"
      },
      "source": [
        "import torch.nn as nn\n",
        "import TestCase as tests\n",
        "\n",
        "class RNN(nn.Module):\n",
        "    \n",
        "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, dropout=0.5):\n",
        "      \n",
        "        super(RNN, self).__init__()\n",
        "        \n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "        \n",
        "        \n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        \n",
        "        \n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers,\n",
        "                            dropout=dropout, batch_first=True)\n",
        "        \n",
        "       \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "       \n",
        "        self.fc = nn.Linear(hidden_dim, output_size)\n",
        "        \n",
        "    \n",
        "    def forward(self, nn_input, hidden):\n",
        "        \n",
        "        nn_input_long = nn_input.long()\n",
        "        embeds = self.embedding(nn_input_long)\n",
        "        \n",
        "        \n",
        "        lstm_out, hidden = self.lstm(embeds,hidden)\n",
        "        \n",
        "       \n",
        "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
        "        \n",
        "        \n",
        "        out = self.dropout(lstm_out)\n",
        "        out = self.fc(out)\n",
        "        \n",
        "        \n",
        "        out = out.view(hidden[0].shape[1], -1, self.output_size)\n",
        "        final_output = out[:,-1]\n",
        "        \n",
        "        \n",
        "       \n",
        "        return final_output, hidden\n",
        "    \n",
        "    \n",
        "    def init_hidden(self, batch_size):\n",
        "        \n",
        "        weight = next(self.parameters()).data\n",
        "        \n",
        "        \n",
        "        if (train_on_gpu):\n",
        "            hidden = (weight.new(self.n_layers,batch_size,self.hidden_dim).zero_().cuda(),\n",
        "                      weight.new(self.n_layers,batch_size,self.hidden_dim).zero_().cuda())\n",
        "        else:\n",
        "            hidden = (weight.new(self.n_layers,batch_size,self.hidden_dim).zero_(),\n",
        "                      weight.new(self.n_layers,batch_size,self.hidden_dim).zero_())\n",
        "        \n",
        "        return hidden\n",
        "\n",
        "tests.test_rnn(RNN, train_on_gpu)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tests Passed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lFJWQc2ay9sQ",
        "outputId": "251bd5a8-012a-4dbf-b6b4-38d291055713"
      },
      "source": [
        "def forward_back_prop(rnn, optimizer, criterion, inp, target, hidden):\n",
        "    if (train_on_gpu):\n",
        "        inp, target = inp.cuda(), target.cuda()\n",
        "    \n",
        "    \n",
        "    hidden = tuple([each.data for each in hidden])\n",
        "    \n",
        "    \n",
        "    rnn.zero_grad()\n",
        "    \n",
        "    \n",
        "    out, hidden = rnn(inp, hidden)\n",
        "    \n",
        "   \n",
        "    loss = criterion(out, target.long())\n",
        "    loss.backward()\n",
        "    \n",
        "    \n",
        "    optimizer.step()\n",
        "\n",
        "    \n",
        "    return loss.item(), hidden\n",
        "\n",
        "tests.test_forward_back_prop(RNN, forward_back_prop, train_on_gpu)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tests Passed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJswTS7Qy_mv"
      },
      "source": [
        "\n",
        "def train_rnn(rnn, batch_size, optimizer, criterion, n_epochs, show_every_n_batches=100):\n",
        "    batch_losses = []\n",
        "    \n",
        "    rnn.train()\n",
        "\n",
        "    print(\"Training for %d epoch(s)...\" % n_epochs)\n",
        "    for epoch_i in range(1, n_epochs + 1):\n",
        "        \n",
        "        \n",
        "        hidden = rnn.init_hidden(batch_size)\n",
        "        \n",
        "        for batch_i, (inputs, labels) in enumerate(train_loader, 1):\n",
        "            \n",
        "           \n",
        "            n_batches = len(train_loader.dataset)//batch_size\n",
        "            if(batch_i > n_batches):\n",
        "                break\n",
        "            \n",
        "            \n",
        "            loss, hidden = forward_back_prop(rnn, optimizer, criterion, inputs, labels, hidden)          \n",
        "           \n",
        "            batch_losses.append(loss)\n",
        "\n",
        "           \n",
        "            if batch_i % show_every_n_batches == 0:\n",
        "                print('Epoch: {:>4}/{:<4}  Loss: {}\\n'.format(\n",
        "                    epoch_i, n_epochs, np.average(batch_losses)))\n",
        "                batch_losses = []\n",
        "\n",
        "    return rnn"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNCRByhjzMS_"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "sequence_length = 10 \n",
        "\n",
        "batch_size = 128\n",
        "\n",
        "\n",
        "train_loader = batch_data(int_text, sequence_length, batch_size)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uHOxNRGazQQz"
      },
      "source": [
        "num_epochs = 9\n",
        "learning_rate = 0.0001\n",
        "vocab_size = len(vocab_to_int) + 1\n",
        "output_size = len(vocab_to_int)\n",
        "embedding_dim = 100\n",
        "hidden_dim = 512\n",
        "n_layers = 2\n",
        "show_every_n_batches = 500"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ZHOVeR6zUFh",
        "outputId": "ec7bcca5-feb4-4131-fbf6-0903286298cb"
      },
      "source": [
        "rnn = RNN(vocab_size, output_size, embedding_dim, hidden_dim, n_layers, dropout=0.5)\n",
        "if train_on_gpu:\n",
        "    rnn.cuda()\n",
        "\n",
        "optimizer = torch.optim.Adam(rnn.parameters(), lr=learning_rate)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "trained_rnn = train_rnn(rnn, batch_size, optimizer, criterion, num_epochs, show_every_n_batches)\n",
        "helper.save_model('trained_rnn', trained_rnn)\n",
        "print('Model Trained and Saved')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training for 9 epoch(s)...\n",
            "Epoch:    1/9     Loss: 5.2582930002212525\n",
            "\n",
            "Epoch:    1/9     Loss: 4.400785830974579\n",
            "\n",
            "Epoch:    1/9     Loss: 4.267888197422027\n",
            "\n",
            "Epoch:    1/9     Loss: 4.199936118602753\n",
            "\n",
            "Epoch:    1/9     Loss: 4.140326580047607\n",
            "\n",
            "Epoch:    1/9     Loss: 4.117083817958831\n",
            "\n",
            "Epoch:    1/9     Loss: 4.0505142855644225\n",
            "\n",
            "Epoch:    1/9     Loss: 3.9987132506370546\n",
            "\n",
            "Epoch:    1/9     Loss: 3.973925922393799\n",
            "\n",
            "Epoch:    1/9     Loss: 3.9416111588478087\n",
            "\n",
            "Epoch:    1/9     Loss: 3.927637899875641\n",
            "\n",
            "Epoch:    1/9     Loss: 3.9087087211608886\n",
            "\n",
            "Epoch:    1/9     Loss: 3.8720106449127196\n",
            "\n",
            "Epoch:    2/9     Loss: 3.8483739580778393\n",
            "\n",
            "Epoch:    2/9     Loss: 3.841087972640991\n",
            "\n",
            "Epoch:    2/9     Loss: 3.8386352286338807\n",
            "\n",
            "Epoch:    2/9     Loss: 3.813948884963989\n",
            "\n",
            "Epoch:    2/9     Loss: 3.8085884914398194\n",
            "\n",
            "Epoch:    2/9     Loss: 3.800582652568817\n",
            "\n",
            "Epoch:    2/9     Loss: 3.7811648926734924\n",
            "\n",
            "Epoch:    2/9     Loss: 3.7660446820259095\n",
            "\n",
            "Epoch:    2/9     Loss: 3.771831813812256\n",
            "\n",
            "Epoch:    2/9     Loss: 3.7585156240463258\n",
            "\n",
            "Epoch:    2/9     Loss: 3.7430667619705202\n",
            "\n",
            "Epoch:    2/9     Loss: 3.7320412583351135\n",
            "\n",
            "Epoch:    2/9     Loss: 3.7444905543327334\n",
            "\n",
            "Epoch:    3/9     Loss: 3.7235279090633333\n",
            "\n",
            "Epoch:    3/9     Loss: 3.72236030292511\n",
            "\n",
            "Epoch:    3/9     Loss: 3.6975070180892944\n",
            "\n",
            "Epoch:    3/9     Loss: 3.7050194907188416\n",
            "\n",
            "Epoch:    3/9     Loss: 3.685415466308594\n",
            "\n",
            "Epoch:    3/9     Loss: 3.7043256726264953\n",
            "\n",
            "Epoch:    3/9     Loss: 3.693717980861664\n",
            "\n",
            "Epoch:    3/9     Loss: 3.683921208381653\n",
            "\n",
            "Epoch:    3/9     Loss: 3.6597331490516662\n",
            "\n",
            "Epoch:    3/9     Loss: 3.6884693632125853\n",
            "\n",
            "Epoch:    3/9     Loss: 3.696482550621033\n",
            "\n",
            "Epoch:    3/9     Loss: 3.6812729687690733\n",
            "\n",
            "Epoch:    3/9     Loss: 3.681449077129364\n",
            "\n",
            "Epoch:    4/9     Loss: 3.6442222617358984\n",
            "\n",
            "Epoch:    4/9     Loss: 3.656955132961273\n",
            "\n",
            "Epoch:    4/9     Loss: 3.6436175470352175\n",
            "\n",
            "Epoch:    4/9     Loss: 3.6480934820175173\n",
            "\n",
            "Epoch:    4/9     Loss: 3.630288489818573\n",
            "\n",
            "Epoch:    4/9     Loss: 3.644951144695282\n",
            "\n",
            "Epoch:    4/9     Loss: 3.6437155184745786\n",
            "\n",
            "Epoch:    4/9     Loss: 3.6306347002983093\n",
            "\n",
            "Epoch:    4/9     Loss: 3.6386331515312196\n",
            "\n",
            "Epoch:    4/9     Loss: 3.6204003515243532\n",
            "\n",
            "Epoch:    4/9     Loss: 3.6204858684539794\n",
            "\n",
            "Epoch:    4/9     Loss: 3.620453811168671\n",
            "\n",
            "Epoch:    4/9     Loss: 3.6181154379844664\n",
            "\n",
            "Epoch:    5/9     Loss: 3.592976123063803\n",
            "\n",
            "Epoch:    5/9     Loss: 3.600482304096222\n",
            "\n",
            "Epoch:    5/9     Loss: 3.59451762676239\n",
            "\n",
            "Epoch:    5/9     Loss: 3.6007536883354185\n",
            "\n",
            "Epoch:    5/9     Loss: 3.5928197355270384\n",
            "\n",
            "Epoch:    5/9     Loss: 3.5981292514801027\n",
            "\n",
            "Epoch:    5/9     Loss: 3.6069934024810792\n",
            "\n",
            "Epoch:    5/9     Loss: 3.5928780303001404\n",
            "\n",
            "Epoch:    5/9     Loss: 3.6010346174240113\n",
            "\n",
            "Epoch:    5/9     Loss: 3.5890593667030335\n",
            "\n",
            "Epoch:    5/9     Loss: 3.585647412776947\n",
            "\n",
            "Epoch:    5/9     Loss: 3.591344557762146\n",
            "\n",
            "Epoch:    5/9     Loss: 3.5811495685577395\n",
            "\n",
            "Epoch:    6/9     Loss: 3.5794414359349584\n",
            "\n",
            "Epoch:    6/9     Loss: 3.5613459830284118\n",
            "\n",
            "Epoch:    6/9     Loss: 3.549064368247986\n",
            "\n",
            "Epoch:    6/9     Loss: 3.560043107032776\n",
            "\n",
            "Epoch:    6/9     Loss: 3.5598376545906065\n",
            "\n",
            "Epoch:    6/9     Loss: 3.5634126505851746\n",
            "\n",
            "Epoch:    6/9     Loss: 3.569252628326416\n",
            "\n",
            "Epoch:    6/9     Loss: 3.5639730072021485\n",
            "\n",
            "Epoch:    6/9     Loss: 3.5609861598014834\n",
            "\n",
            "Epoch:    6/9     Loss: 3.560404938220978\n",
            "\n",
            "Epoch:    6/9     Loss: 3.571547365665436\n",
            "\n",
            "Epoch:    6/9     Loss: 3.560738221168518\n",
            "\n",
            "Epoch:    6/9     Loss: 3.555408555984497\n",
            "\n",
            "Epoch:    7/9     Loss: 3.5444448284566463\n",
            "\n",
            "Epoch:    7/9     Loss: 3.545247624874115\n",
            "\n",
            "Epoch:    7/9     Loss: 3.5480025358200074\n",
            "\n",
            "Epoch:    7/9     Loss: 3.543678176403046\n",
            "\n",
            "Epoch:    7/9     Loss: 3.533741976737976\n",
            "\n",
            "Epoch:    7/9     Loss: 3.5528797187805177\n",
            "\n",
            "Epoch:    7/9     Loss: 3.526290533065796\n",
            "\n",
            "Epoch:    7/9     Loss: 3.5410885577201845\n",
            "\n",
            "Epoch:    7/9     Loss: 3.5307955856323243\n",
            "\n",
            "Epoch:    7/9     Loss: 3.5281861896514894\n",
            "\n",
            "Epoch:    7/9     Loss: 3.5431795711517333\n",
            "\n",
            "Epoch:    7/9     Loss: 3.525634047985077\n",
            "\n",
            "Epoch:    7/9     Loss: 3.5276952257156373\n",
            "\n",
            "Epoch:    8/9     Loss: 3.533438755631816\n",
            "\n",
            "Epoch:    8/9     Loss: 3.5121354269981384\n",
            "\n",
            "Epoch:    8/9     Loss: 3.5114419083595276\n",
            "\n",
            "Epoch:    8/9     Loss: 3.4989762506484987\n",
            "\n",
            "Epoch:    8/9     Loss: 3.511924197673798\n",
            "\n",
            "Epoch:    8/9     Loss: 3.5152949476242066\n",
            "\n",
            "Epoch:    8/9     Loss: 3.519263397693634\n",
            "\n",
            "Epoch:    8/9     Loss: 3.5235161843299867\n",
            "\n",
            "Epoch:    8/9     Loss: 3.506601700305939\n",
            "\n",
            "Epoch:    8/9     Loss: 3.505688479423523\n",
            "\n",
            "Epoch:    8/9     Loss: 3.5072060513496397\n",
            "\n",
            "Epoch:    8/9     Loss: 3.513620319366455\n",
            "\n",
            "Epoch:    8/9     Loss: 3.5254687209129334\n",
            "\n",
            "Epoch:    9/9     Loss: 3.5048599105378297\n",
            "\n",
            "Epoch:    9/9     Loss: 3.4903665804862976\n",
            "\n",
            "Epoch:    9/9     Loss: 3.5036808824539185\n",
            "\n",
            "Epoch:    9/9     Loss: 3.5025419268608093\n",
            "\n",
            "Epoch:    9/9     Loss: 3.4948595886230467\n",
            "\n",
            "Epoch:    9/9     Loss: 3.4894936485290526\n",
            "\n",
            "Epoch:    9/9     Loss: 3.4856624689102174\n",
            "\n",
            "Epoch:    9/9     Loss: 3.488397946834564\n",
            "\n",
            "Epoch:    9/9     Loss: 3.489559745311737\n",
            "\n",
            "Epoch:    9/9     Loss: 3.4890885882377622\n",
            "\n",
            "Epoch:    9/9     Loss: 3.4913776998519896\n",
            "\n",
            "Epoch:    9/9     Loss: 3.5014772176742555\n",
            "\n",
            "Epoch:    9/9     Loss: 3.5068252773284914\n",
            "\n",
            "Model Trained and Saved\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aVIiyqrH-uxE"
      },
      "source": [
        "import signal\n",
        "\n",
        "from contextlib import contextmanager\n",
        "\n",
        "import requests\n",
        "\n",
        "\n",
        "DELAY = INTERVAL = 4 * 60  # interval time in seconds\n",
        "MIN_DELAY = MIN_INTERVAL = 2 * 60\n",
        "KEEPALIVE_URL = \"https://nebula.udacity.com/api/v1/remote/keep-alive\"\n",
        "TOKEN_URL = \"http://metadata.google.internal/computeMetadata/v1/instance/attributes/keep_alive_token\"\n",
        "TOKEN_HEADERS = {\"Metadata-Flavor\":\"Google\"}\n",
        "\n",
        "\n",
        "def _request_handler(headers):\n",
        "    def _handler(signum, frame):\n",
        "        requests.request(\"POST\", KEEPALIVE_URL, headers=headers)\n",
        "    return _handler\n",
        "\n",
        "\n",
        "@contextmanager\n",
        "def active_session(delay=DELAY, interval=INTERVAL):\n",
        "    \n",
        "    token = requests.request(\"GET\", TOKEN_URL, headers=TOKEN_HEADERS).text\n",
        "    headers = {'Authorization': \"STAR \" + token}\n",
        "    delay = max(delay, MIN_DELAY)\n",
        "    interval = max(interval, MIN_INTERVAL)\n",
        "    original_handler = signal.getsignal(signal.SIGALRM)\n",
        "    try:\n",
        "        signal.signal(signal.SIGALRM, _request_handler(headers))\n",
        "        signal.setitimer(signal.ITIMER_REAL, delay, interval)\n",
        "        yield\n",
        "    finally:\n",
        "        signal.signal(signal.SIGALRM, original_handler)\n",
        "        signal.setitimer(signal.ITIMER_REAL, 0)\n",
        "\n",
        "\n",
        "def keep_awake(iterable, delay=DELAY, interval=INTERVAL):\n",
        "  \n",
        "    with active_session(delay, interval): yield from iterable"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDZOLp99-3bH"
      },
      "source": [
        "import torch\n",
        "import helper\n",
        "import TestCase as tests\n",
        "\n",
        "_, vocab_to_int, int_to_vocab, token_dict = helper.load_preprocess()\n",
        "trained_rnn = helper.load_model('trained_rnn')"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Zsy4FHi_kda"
      },
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def generate(rnn, prime_id, int_to_vocab, token_dict, pad_value, predict_len=100):\n",
        "    \n",
        "    rnn.eval()\n",
        "    \n",
        "   \n",
        "    current_seq = np.full((1, sequence_length), pad_value)\n",
        "    current_seq[-1][-1] = prime_id\n",
        "    predicted = [int_to_vocab[prime_id]]\n",
        "    \n",
        "    for _ in range(predict_len):\n",
        "        if train_on_gpu:\n",
        "            current_seq = torch.LongTensor(current_seq).cuda()\n",
        "        else:\n",
        "            current_seq = torch.LongTensor(current_seq)\n",
        "        \n",
        "        \n",
        "        hidden = rnn.init_hidden(current_seq.size(0))\n",
        "        \n",
        "        \n",
        "        output, _ = rnn(current_seq, hidden)\n",
        "        \n",
        "        # get the next word probabilities\n",
        "        p = F.softmax(output, dim=1).data\n",
        "        if(train_on_gpu):\n",
        "            p = p.cpu() \n",
        "         \n",
        "        \n",
        "        top_k = 5\n",
        "        p, top_i = p.topk(top_k)\n",
        "        top_i = top_i.numpy().squeeze()\n",
        "        \n",
        "        \n",
        "        p = p.numpy().squeeze()\n",
        "        word_i = np.random.choice(top_i, p=p/p.sum())\n",
        "        \n",
        "        \n",
        "        word = int_to_vocab[word_i]\n",
        "        predicted.append(word)\n",
        "        \n",
        "        if(train_on_gpu):\n",
        "            current_seq = current_seq.cpu()\n",
        "        \n",
        "        \n",
        "        if(train_on_gpu):\n",
        "            current_seq = current_seq.cpu() \n",
        "            \n",
        "        current_seq = np.roll(current_seq, -1, 1)\n",
        "        current_seq[-1][-1] = word_i\n",
        "    \n",
        "    gen_sentences = ' '.join(predicted)\n",
        "    for key, token in token_dict.items():\n",
        "        ending = ' ' if key in ['\\n', '(', '\"'] else ''\n",
        "        gen_sentences = gen_sentences.replace(' ' + token.lower(), key)\n",
        "    gen_sentences = gen_sentences.replace('\\n ', '\\n')\n",
        "    gen_sentences = gen_sentences.replace('( ', '(')\n",
        "    return gen_sentences"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ejzCMSPu_7m0",
        "outputId": "7b783b5c-aec5-45cb-95db-4b11a38825a3"
      },
      "source": [
        "import numpy as np\n",
        "import helper\n",
        "\n",
        "\n",
        "gen_length = 200 \n",
        "prime_word = 'kramer' \n",
        "pad_word = helper.SPECIAL_WORDS['PADDING']\n",
        "generated_script = generate(trained_rnn, vocab_to_int[prime_word + ':'], int_to_vocab, token_dict, vocab_to_int[pad_word], gen_length)\n",
        "print(generated_script)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "kramer:.\n",
            "\n",
            "kramer:(from) oh, no, it's a lot of.\n",
            "\n",
            "george:(on) oh, i don't have to be in a little, but i don't think i was just gonna have to get it in there.\n",
            "\n",
            "jerry:(to jerry) you know what i mean about the one of a show?\n",
            "\n",
            "jerry: no.\n",
            "\n",
            "elaine: well, i think i have a show of the time in the ya. i mean, i know that i was just in my life.\n",
            "\n",
            "george: i can't believe i don't have to see the other of my apartment, the.\n",
            "\n",
            "elaine: well, i don't think i can.\n",
            "\n",
            "morty: oh, no, no. no, no.\n",
            "\n",
            "elaine: what?\n",
            "\n",
            "jerry: well i don't know, i was gonna go back to the way to the other.\n",
            "\n",
            "george: oh, yeah.\n",
            "\n",
            "kramer: yeah.\n",
            "\n",
            "kramer: well...\n",
            "\n",
            "george:(looking up) well, you\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IBtaoSkJAMDf"
      },
      "source": [
        "f =  open(\"generated_script_1.txt\",\"w\")\n",
        "f.write(generated_script)\n",
        "f.close()"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mB3bE7GGATYz",
        "outputId": "5bfa3936-2a54-4fae-a453-739e33111c94"
      },
      "source": [
        "import nltk\n",
        "\n",
        "hypothesis = text.split()[:120]\n",
        "reference = generated_script.split()\n",
        "#the maximum is bigram, so assign the weight into 2 half.\n",
        "BLEUscore = nltk.translate.bleu_score.sentence_bleu([reference], hypothesis, weights = (0.5, 0.5))\n",
        "print(BLEUscore)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.08142544724996532\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "unSF2EAdAYKV",
        "outputId": "a6a7be86-3f8a-4a94-d8ee-856c7e8f146f"
      },
      "source": [
        "from rouge_score import rouge_scorer\n",
        "\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
        "scores = scorer.score(text[:600], generated_script)\n",
        "scores"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'rouge1': Score(precision=0.17857142857142858, recall=0.20491803278688525, fmeasure=0.19083969465648853),\n",
              " 'rougeL': Score(precision=0.10714285714285714, recall=0.12295081967213115, fmeasure=0.11450381679389313)}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03xs7MDFBCPz"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3eyCmXmIBusS",
        "outputId": "5b7d8a79-9d5f-432b-8f68-ede9b0d9b971"
      },
      "source": [
        "!pyminifier -O -o tv_script_generator --gzip TV_Script.py"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TV_Script.py (10994) reduced to 4176 bytes (37.98% of original size)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}